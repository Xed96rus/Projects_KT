# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DqhVLHg_LzFPPtXS_7hFvTG339aQnvTh
"""



import numpy as np
import matplotlib.pyplot as plt

# Класс для дуальных чисел
class DualNumber:
    def __init__(self, real, dual=0):
        self.real = real  # Вещественная часть
        self.dual = dual  # Дуальная часть (производная)

    # Переопределение операций для дуальных чисел
    def __add__(self, other):
        if isinstance(other, DualNumber):
            return DualNumber(self.real + other.real, self.dual + other.dual)

        return DualNumber(self.real + other, self.dual)

    def __sub__(self, other):
        if isinstance(other, DualNumber):
            return DualNumber(self.real - other.real, self.dual - other.dual)

        return DualNumber(self.real - other, self.dual)

    def __mul__(self, other):
        if isinstance(other, DualNumber):
            return DualNumber(self.real * other.real, self.real * other.dual + self.dual * other.real)

        return DualNumber(self.real * other, self.dual * other)

    def __truediv__(self, other):
        if isinstance(other, DualNumber):
            return DualNumber(self.real / other.real, (self.dual * other.real - self.real * other.dual) / (other.real ** 2))

            return DualNumber(self.real / other, self.dual / other)

    def __pow__(self, power):
        return DualNumber(self.real ** power, power * self.real ** (power - 1) * self.dual)

    def __radd__(self, other):
        return self.__add__(other)

    def __rsub__(self, other):
        return DualNumber(other - self.real, -self.dual)

    def __rmul__(self, other):
        return self.__mul__(other)

    def __rtruediv__(self, other):
        return DualNumber(other / self.real, -other * self.dual / (self.real ** 2))

    def __repr__(self):
        return f"DualNumber(real={self.real}, dual={self.dual})"

# Функция для вычисления среднеквадратичной ошибки
def mean_squared_error(y_true, y_predicted):
    return np.sum((y_true - y_predicted)**2) / len(y_true)

# Функция градиентного спуска с использованием дуальных чисел
def gradient_descent(xs, ys, n_iter=1000, l_rate=0.0001, stop_threshold=1e-8, print_iters=True, visualize=True):
    q = xs.shape[1]
    current_weight = np.array([DualNumber(0.1) for _ in range(q)])
    current_bias = DualNumber(0.01)
    N = len(xs)

    losses = []
    weights = []
    bias = []
    previous_loss = None

    for i in range(n_iter):
        ys_predicted = np.array([sum(x * w for x, w in zip(x_row, current_weight)) + current_bias for x_row in xs])
        current_loss = mean_squared_error(ys, np.array([y.real for y in ys_predicted]))

        if previous_loss and np.abs(previous_loss - current_loss) <= stop_threshold:
            break

        previous_loss = current_loss
        losses.append(current_loss)
        weights.append(np.array([w.real for w in current_weight]))
        bias.append(current_bias.real)

        weight_gradient = np.array([DualNumber(0) for _ in range(q)])
        bias_gradient = DualNumber(0)

        for x_row, y_true in zip(xs, ys):
            y_pred = sum(x * w for x, w in zip(x_row, current_weight)) + current_bias
            error = y_pred - DualNumber(y_true)
            for j in range(q):
                weight_gradient[j] += error * x_row[j]
            bias_gradient += error

        weight_gradient = (2/N) * weight_gradient
        bias_gradient = (2/N) * bias_gradient

        # Ограничиваем величину градиента для смещения
        if abs(bias_gradient.real) > 1.0:
            bias_gradient = DualNumber(bias_gradient.real / abs(bias_gradient.real), bias_gradient.dual)

        current_weight = current_weight - (l_rate * weight_gradient)
        current_bias = current_bias - (l_rate * bias_gradient)

        if print_iters:
            print(f"Итерация {i+1}: Стоимость {current_loss}, Вес: {[w.real for w in current_weight]}, Отклонение: {current_bias.real}")

    if visualize:
        plt.figure(figsize=(10, 6))
        plt.plot(range(len(losses)), losses, marker='o', linestyle='-', color='r')
        plt.title("Изменение функции потерь")
        plt.xlabel("Итерация")
        plt.ylabel("Погрешность (Функция потерь)")
        plt.grid(True)
        plt.show()

    return (np.array([w.real for w in current_weight]), current_bias.real)
xs = np.array([[1,3], [4,5], [8,10] ])
ys = np.array([[160], [170], [190]])
res = gradient_descent(xs, ys, n_iter=2000, l_rate=0.0001, stop_threshold=1e-8, print_iters=True)